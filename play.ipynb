{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "TQDM_ON = True\n",
    "if TQDM_ON:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "z = gzip.open(\"train.json.gz\")\n",
    "\n",
    "dataset = []\n",
    "for l in z:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userID': 'u70666506',\n",
       " 'early_access': False,\n",
       " 'hours': 63.5,\n",
       " 'hours_transformed': 6.011227255423254,\n",
       " 'found_funny': 1,\n",
       " 'text': 'If you want to sit in queue for 10-20min and have 140 ping then this game is perfect for you :)',\n",
       " 'gameID': 'g49368897',\n",
       " 'user_id': '76561198030408772',\n",
       " 'date': '2017-05-20'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MSE(y, ypred):\n",
    "#     if isinstance(y,numpy.ndarray):\n",
    "#         y = y.reshape((-1,))\n",
    "#     if isinstance(ypred,numpy.ndarray):\n",
    "#         ypred = ypred.reshape((-1,))\n",
    "#     if len(y)!=len(ypred):\n",
    "#         raise ValueError(\"len(y) don't equal len(ypred)\")\n",
    "#     sq_err_sum = 0.0\n",
    "#     for yl,yp in zip(y,ypred):\n",
    "#         sq_err_sum+=(yl-yp)**2\n",
    "#     return sq_err_sum/len(y)\n",
    "\n",
    "# def MAE(y, ypred):\n",
    "#     if isinstance(y,numpy.ndarray):\n",
    "#         y = y.reshape((-1,))\n",
    "#     if isinstance(ypred,numpy.ndarray):\n",
    "#         ypred = ypred.reshape((-1,))\n",
    "#     if len(y)!=len(ypred):\n",
    "#         raise ValueError(\"len(y) don't equal len(ypred)\")\n",
    "#     abs_err_sum = 0.0\n",
    "#     for yl,yp in zip(y,ypred):\n",
    "#         abs_err_sum+=abs(yl-yp)\n",
    "#     return abs_err_sum/len(y)\n",
    "\n",
    "def MSE(y, ypred):\n",
    "    assert isinstance(y,numpy.ndarray)\n",
    "    assert isinstance(ypred,numpy.ndarray)\n",
    "    assert y.shape==ypred.shape\n",
    "    sq_err = (y-ypred)**2\n",
    "    return numpy.mean(sq_err)\n",
    "\n",
    "def MAE(y, ypred):\n",
    "    assert isinstance(y,numpy.ndarray)\n",
    "    assert isinstance(ypred,numpy.ndarray)\n",
    "    assert y.shape==ypred.shape\n",
    "    abs_err = numpy.abs(y-ypred)\n",
    "    return numpy.mean(abs_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataset:\n",
    "    u,i = d['userID'],d['gameID']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    reviewsPerItem[i].append(d)\n",
    "    \n",
    "for u in reviewsPerUser:\n",
    "    reviewsPerUser[u].sort(key=lambda x: x['date'])\n",
    "    \n",
    "for i in reviewsPerItem:\n",
    "    reviewsPerItem[i].sort(key=lambda x: x['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feat1(d):\n",
    "    return [1.0,float(d['hours'])]\n",
    "\n",
    "X = list(feat1(d) for d in dataset)\n",
    "X = numpy.array(X)\n",
    "y = list(float(len(d['text'])) for d in dataset)\n",
    "y = numpy.array(y)\n",
    "y = y.reshape((-1,1))\n",
    "mod = linear_model.LinearRegression()\n",
    "mod.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1 = mod.coef_[0][1]\n",
    "ypred = mod.predict(X)\n",
    "mse_q1 = MSE(y,ypred)\n",
    "answers['Q1'] = [theta_1, mse_q1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': [0.007857269704336025, 570936.2842458971]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_pipeline(get_xy_func):\n",
    "    Xs = list()\n",
    "    ys = list()\n",
    "    for d in dataset:\n",
    "        x1,y1 = get_xy_func(d)\n",
    "        Xs.append(x1)\n",
    "        ys.append(y1)\n",
    "    Xs = numpy.array(Xs,dtype=float)\n",
    "    if len(Xs.shape)==1:\n",
    "        Xs = Xs.reshape((-1,1))\n",
    "    ys = numpy.array(ys,dtype=float)\n",
    "    print(\"average of y:\"+str(numpy.average(ys)))\n",
    "    print(\"variance of y:\"+str(numpy.var(ys,ddof=1)))\n",
    "    if len(ys.shape)==1:\n",
    "        ys = ys.reshape((-1,1))\n",
    "    model = linear_model.LinearRegression(fit_intercept=False)\n",
    "    model.fit(Xs,ys)\n",
    "    ypred = model.predict(Xs)\n",
    "    return model,MSE(y,ypred),MAE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_transform(t):\n",
    "    return math.log2(t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of y:390.9600857142857\n",
      "variance of y:570944.2224938355\n"
     ]
    }
   ],
   "source": [
    "all_hours = list(d['hours'] for d in dataset)\n",
    "\n",
    "def calculate_median(l):\n",
    "    sorted_l = sorted(l)\n",
    "    list_length = len(sorted_l)   \n",
    "    if list_length == 0:\n",
    "        return None\n",
    "    if list_length % 2 == 1:\n",
    "        median = sorted_l[list_length // 2]\n",
    "    else:\n",
    "        m1 = sorted_l[list_length // 2 - 1]\n",
    "        m2 = sorted_l[list_length // 2]\n",
    "        median = (m1 + m2) / 2\n",
    "    return median\n",
    "\n",
    "median_play_time = calculate_median(all_hours)\n",
    "\n",
    "def get_xy_2(d):\n",
    "    l = float(len(d['text']))\n",
    "    t = float(d['hours'])\n",
    "    return [1.0,t,t_transform(t),math.sqrt(t),int(t>median_play_time)],l\n",
    "\n",
    "mod2,mse2,mae2 = lin_reg_pipeline(get_xy_2)\n",
    "answers['Q2'] = mse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average of y:390.9600857142857\n",
      "variance of y:570944.2224938355\n"
     ]
    }
   ],
   "source": [
    "def get_xy_3(d):\n",
    "    l = float(len(d['text']))\n",
    "    t = float(d['hours'])\n",
    "    x1 = list()\n",
    "    x1.append(1.0)\n",
    "    for t_ref in [1,5,10,100,1000]:\n",
    "        x1.append(int(t>t_ref))\n",
    "    return x1,l\n",
    "\n",
    "mod3,mse3,mae3 = lin_reg_pipeline(get_xy_3)\n",
    "answers['Q3'] = mse3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_xy_4(d):\n",
    "#     l = float(len(d['text']))\n",
    "#     t = float(d['hours'])\n",
    "#     return [1.0,l],t\n",
    "\n",
    "# mod4,mse4,mae4 = lin_reg_pipeline(get_xy_4)\n",
    "# answers['Q4'] = [mse4, mae4, \"mae is better, because review_len and time_played are not so relevant, the mse of the model is extremly big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat4(d):\n",
    "    return [1.0,float(len(d['text']))]\n",
    "\n",
    "X = [feat4(d) for d in dataset]\n",
    "X = numpy.array(X)\n",
    "y = [[float(d['hours'])] for d in dataset]\n",
    "y = numpy.array(y)\n",
    "\n",
    "mod = linear_model.LinearRegression(fit_intercept=False)\n",
    "mod.fit(X,y)\n",
    "predictions = mod.predict(X)\n",
    "\n",
    "mse = MSE(y,predictions)\n",
    "mae = MAE(y,predictions)\n",
    "answers['Q4'] = [mse, mae, \"mae is better, because review_len and time_played are not so relevant, the mse of the model is extremly big\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trans = numpy.vectorize(t_transform)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LinearRegression(fit_intercept=False)\n",
    "mod.fit(X,y_trans)\n",
    "predictions_trans = mod.predict(X)\n",
    "mod5 = mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_trans = MSE(y_trans,predictions_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_untrans =  MSE(y,2**predictions_trans-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [mse_trans, mse_untrans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1hot(l):\n",
    "    res = list()\n",
    "    for _ in range(l):\n",
    "        res.append(0)\n",
    "    return res\n",
    "        \n",
    "\n",
    "def feat6(d):\n",
    "    h = float(d['hours'])\n",
    "    int_h = int(h)\n",
    "    if int_h>=99:\n",
    "        int_h=99\n",
    "    res = get_1hot(100)\n",
    "    res[int_h]=1.0\n",
    "    return res\n",
    "    \n",
    "X = [feat6(d) for d in dataset]\n",
    "X = numpy.array(X)\n",
    "y = [len(d['text']) for d in dataset]\n",
    "y = numpy.array(y)\n",
    "y=y.reshape((-1,1))\n",
    "Xtrain, Xvalid, Xtest = X[:len(X)//2], X[len(X)//2:(3*len(X))//4], X[(3*len(X))//4:]\n",
    "ytrain, yvalid, ytest = y[:len(X)//2], y[len(X)//2:(3*len(X))//4], y[(3*len(X))//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "mses = {}\n",
    "bestC = None\n",
    "\n",
    "for c in [1, 10, 100, 1000, 10000]:\n",
    "    model = linear_model.Ridge(alpha=float(c))\n",
    "    model.fit(Xtrain,ytrain)\n",
    "    models[c] = model\n",
    "    mse_valid = MSE(yvalid,model.predict(Xvalid))\n",
    "    mses[c] = mse_valid\n",
    "    if bestC==None:\n",
    "        bestC = c\n",
    "    else:\n",
    "        if mse_valid<mses[bestC]:\n",
    "            bestC = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_valid = mses[bestC]\n",
    "mse_test = MSE(ytest,model.predict(Xtest))\n",
    "answers['Q6'] = [bestC, mse_valid, mse_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    d['hours_transformed'] = t_transform(d['hours'])\n",
    "times = [d['hours_transformed'] for d in dataset]\n",
    "median = statistics.median(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_1h_cnt = 0\n",
    "for d in dataset:\n",
    "    if d['hours']<1.0:\n",
    "        less_than_1h_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [median, less_than_1h_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat8(d):\n",
    "    return [1.0,float(len(d['text']))]\n",
    "X = [feat8(d) for d in dataset]\n",
    "y = [d['hours_transformed'] > median for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(class_weight='balanced')\n",
    "mod.fit(X,y)\n",
    "predictions = mod.predict(X) # Binary vector of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_info(y,predictions):\n",
    "    y = numpy.array(y,dtype=int)\n",
    "    y = y.reshape((-1,))\n",
    "    predictions = numpy.array(predictions,dtype=int)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    # print(y_actual)\n",
    "    # print(y_predict)\n",
    "    TP = numpy.sum((y == 1) & (predictions == 1))\n",
    "    FP = numpy.sum((y == 0) & (predictions == 1))\n",
    "    TN = numpy.sum((y == 0) & (predictions == 0))\n",
    "    FN = numpy.sum((y == 1) & (predictions == 0))\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (TP + FN)\n",
    "    BER = 1 - (0.5 * (TPR + TNR))\n",
    "    return TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER = get_performance_info(y,predictions)\n",
    "answers['Q8'] = [TP, TN, FP, FN, BER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': [0.007857269704336025, 570936.2842458971],\n",
       " 'Q2': 565419.5340402178,\n",
       " 'Q3': 565405.4395885819,\n",
       " 'Q4': [75735.70018272949,\n",
       "  90.35613031985204,\n",
       "  'mae is better, because review_len and time_played are not so relevant, the mse of the model is extremly big'],\n",
       " 'Q5': [5.255254235328314, 78668.56502956731],\n",
       " 'Q6': [1000, 581432.8208480754, 562824.2275086499],\n",
       " 'Q7': [3.4724877714627436, 19913],\n",
       " 'Q8': [24656, 67811, 20007, 62526, 0.4725063905614679]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Q9 Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = mod.decision_function(X)\n",
    "score_labels = list(zip(scores,y))\n",
    "score_labels.sort(reverse=True)\n",
    "sorted_labels = [tup[1] for tup in score_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_q9 = list()\n",
    "for k in [5,10,100,1000]:\n",
    "    ths = score_labels[k-1][0]\n",
    "    l,r=k-1,len(score_labels)-1\n",
    "    while l<r:\n",
    "        m=(l+r+1)//2\n",
    "        confidence_m = score_labels[m][0]\n",
    "        if confidence_m<ths:\n",
    "            r=m-1\n",
    "        else:\n",
    "            l=m\n",
    "    k_actual=l+1\n",
    "    ans_q9.append(sum(sorted_labels[:k_actual])/k_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q9\"]=ans_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod5_regression_y = mod5.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_max(l,r,iter_times,each_split,cal_performance_func):\n",
    "    iter_range = range(iter_times)\n",
    "    if TQDM_ON:\n",
    "        iter_range = tqdm(iter_range)\n",
    "    for i in iter_range:\n",
    "        thsld_with_performance = list()\n",
    "        split_i_range = range(1,each_split)\n",
    "        each_split_size = (r-l)/each_split\n",
    "        if TQDM_ON:\n",
    "            split_i_range = tqdm(split_i_range)\n",
    "        for s_i in split_i_range:\n",
    "            x_s_i = l+each_split_size*s_i\n",
    "            # my_pred_play_model2(x_s_i)\n",
    "            # accu_this = calculate_pred_play_accu()\n",
    "            performance_this = cal_performance_func(x_s_i)\n",
    "            thsld_with_performance.append((x_s_i,performance_this))\n",
    "            thsld_with_performance.sort(key=lambda tup:tup[1],reverse=True)\n",
    "            ths_max_performance = thsld_with_performance[0][0]\n",
    "            performance = thsld_with_performance[0][1]\n",
    "            l,r = ths_max_performance-each_split_size,ths_max_performance+each_split_size\n",
    "    return (ths_max_performance,performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(ths):\n",
    "    ypred = mod5_regression_y>ths\n",
    "    return -get_performance_info(y,ypred)[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 88.56it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 104.05it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 119.64it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 130.48it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 120.16it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 119.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 20.12it/s]\n"
     ]
    }
   ],
   "source": [
    "ths_best,ber_best_neg=search_max(0.0,10.0,6,6,calculate_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q10\"] = [ths_best,-ber_best_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q11 code\n",
    "dataTrain = dataset[:int(len(dataset)*0.9)]\n",
    "dataTest = dataset[int(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userMedian = defaultdict(list)\n",
    "itemMedian = defaultdict(list)\n",
    "\n",
    "dataTrain[0]\n",
    "\n",
    "for d in dataTrain:\n",
    "    uid,item_id,h = d[\"userID\"],d[\"gameID\"],d[\"hours\"]\n",
    "    userMedian[uid].append(h)\n",
    "    itemMedian[item_id].append(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in userMedian:\n",
    "    userMedian[u] = statistics.median(userMedian[u])\n",
    "\n",
    "for i in itemMedian:\n",
    "    itemMedian[i] = statistics.median(itemMedian[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q11'] = [itemMedian['g35322304'], userMedian['u55351001']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times_train = list(d['hours'] for d in dataTrain)\n",
    "global_median = statistics.median(all_times_train)\n",
    "\n",
    "def f12(u,i):\n",
    "    if i in itemMedian:\n",
    "        return int(itemMedian[i]>global_median)\n",
    "    if u in userMedian:\n",
    "        return int(userMedian[u]>global_median)\n",
    "    return 0\n",
    "\n",
    "preds = [f12(d['userID'], d['gameID']) for d in dataTest]\n",
    "y = [int(d['hours']>global_median) for d in dataTest]\n",
    "correct_cnt = 0\n",
    "for yl,yp in zip(y,preds):\n",
    "    if yl==yp:\n",
    "        correct_cnt+=1\n",
    "accuracy = correct_cnt/len(y)\n",
    "answers['Q12'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "rating_dict = {}\n",
    "\n",
    "for d in dataset:\n",
    "    user,item,tt = d['userID'], d['gameID'],d['hours_transformed']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    rating_dict[(user,item)]=tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(i, func, N):\n",
    "    item_with_sim = list()\n",
    "    for j in usersPerItem:\n",
    "        if j==i:\n",
    "            continue\n",
    "        sim_j=func(i,j)\n",
    "        item_with_sim.append((sim_j,j))\n",
    "    item_with_sim.sort(key=lambda tup:tup[0],reverse=True)\n",
    "    return item_with_sim[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(i,j):\n",
    "    si = usersPerItem[i]\n",
    "    sj = usersPerItem[j]\n",
    "    return len(si.intersection(sj))/len(si.union(sj))\n",
    "\n",
    "\n",
    "\n",
    "def cos_sim_14(i,j):\n",
    "    si = usersPerItem[i]\n",
    "    sj = usersPerItem[j]\n",
    "    i_norm = math.sqrt(len(si))\n",
    "    j_norm = math.sqrt(len(sj))\n",
    "    numerator = 0\n",
    "    for shared_u in si.intersection(sj):\n",
    "        numerator+=(1 if rating_dict[(shared_u,i)]>global_median else -1)*(1 if rating_dict[(shared_u,j)]>global_median else -1)\n",
    "    return numerator/(i_norm*j_norm)\n",
    "\n",
    "def cos_sim(i,j):\n",
    "    si = usersPerItem[i]\n",
    "    sj = usersPerItem[j]\n",
    "    i_norm = math.sqrt(sum(rating_dict[(u,i)]**2 for u in si))\n",
    "    j_norm = math.sqrt(sum(rating_dict[(u,j)]**2 for u in sj))\n",
    "    numerator = 0.0\n",
    "    for shared_u in si.intersection(sj):\n",
    "        numerator+=rating_dict[(shared_u,i)]*rating_dict[(shared_u,j)]\n",
    "    return numerator/(i_norm*j_norm)\n",
    "\n",
    "ms = mostSimilar(dataset[0]['gameID'], jaccard, 10)\n",
    "answers['Q13'] = [ms[0][0], ms[-1][0]]\n",
    "\n",
    "rating_dict = {}\n",
    "for d in dataset:\n",
    "    user,item,h = d['userID'], d['gameID'],d['hours']\n",
    "    rating_dict[(user,item)] = 1 if h>global_median else -1\n",
    "\n",
    "ms = mostSimilar(dataset[0]['gameID'], cos_sim, 10)\n",
    "answers['Q14'] = [ms[0][0], ms[-1][0]]\n",
    "\n",
    "rating_dict = {}\n",
    "for d in dataset:\n",
    "    user,item,h = d['userID'], d['gameID'],d['hours_transformed']\n",
    "    rating_dict[(user,item)] = h\n",
    "\n",
    "ms = mostSimilar(dataset[0]['gameID'], cos_sim, 10)\n",
    "answers['Q15'] = [ms[0][0], ms[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': [0.007857269704336025, 570936.2842458971],\n",
       " 'Q2': 565419.5340402178,\n",
       " 'Q3': 565405.4395885819,\n",
       " 'Q4': [75735.70018272949,\n",
       "  90.35613031985204,\n",
       "  'mae is better, because review_len and time_played are not so relevant, the mse of the model is extremly big'],\n",
       " 'Q5': [5.255254235328314, 78668.56502956731],\n",
       " 'Q6': [1000, 581432.8208480754, 562824.2275086499],\n",
       " 'Q7': [3.4724877714627436, 19913],\n",
       " 'Q8': [24656, 67811, 20007, 62526, 0.4725063905614679],\n",
       " 'Q11': [0.5, 3.9],\n",
       " 'Q12': 0.7410857142857142,\n",
       " 'Q13': [0.07988165680473373, 0.04390243902439024],\n",
       " 'Q14': [0.10251693271055495, 0.061667331307041336],\n",
       " 'Q15': [0.3301567230633554, 0.12290154232706592]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_midterm.txt\", 'w+')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
